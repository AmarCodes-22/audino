{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amar/projects/audino/encoder/audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.\n",
      "  warn(\"Unable to import 'webrtcvad'. This package enables noise removal and is recommended.\")\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "from encoder.params_model import model_embedding_size as speaker_embedding_size\n",
    "from synthesizer.inference import Synthesizer\n",
    "from encoder import inference as encoder\n",
    "from vocoder import inference as vocoder \n",
    "\n",
    "# maybe\n",
    "# import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args in demo_cli.py\n",
    "# paths to respective .pt files\n",
    "project_root = os.getcwd()\n",
    "enc_model_fpath = os.path.join(project_root, 'pretrained', 'encoder', 'saved_models', 'pretrained.pt')\n",
    "syn_model_fpath = os.path.join(project_root, 'pretrained', 'synthesizer', 'saved_models', 'pretrained', 'pretrained.pt')\n",
    "voc_model_fpath = os.path.join(project_root, 'pretrained', 'vocoder', 'saved_models', 'pretrained', 'pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesizer using device: cpu\n",
      "Building Wave-RNN\n",
      "Trainable Parameters: 4.481M\n",
      "Loading model weights at /home/amar/projects/audino/pretrained/vocoder/saved_models/pretrained/pretrained.pt\n"
     ]
    }
   ],
   "source": [
    "# load the models\n",
    "encoder.load_model(enc_model_fpath)\n",
    "synthesizer = Synthesizer(syn_model_fpath)\n",
    "vocoder.load_model(voc_model_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 30.870M\n",
      "+----------+---+\n",
      "| Tacotron | r |\n",
      "+----------+---+\n",
      "|   295k   | 2 |\n",
      "+----------+---+\n",
      " \n",
      "\n",
      "| Generating 1/1\n",
      "\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "       -3.73963522e-07, -1.87383031e-07, -0.00000000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing encoder\n",
    "encoder.embed_utterance(np.zeros(encoder.sampling_rate))\n",
    "\n",
    "# create a dummy encoding\n",
    "embed = np.random.rand(speaker_embedding_size)\n",
    "\n",
    "# Embeddings are L2-normalized\n",
    "embed /= np.linalg.norm(embed)\n",
    "\n",
    "# synthesizer can handle multiple inputs with batching so lets create another embedding\n",
    "embeds = [embed, np.zeros(speaker_embedding_size)]\n",
    "texts = ['test_1', 'test_2']\n",
    "\n",
    "mels = synthesizer.synthesize_spectrograms(texts, embeds)\n",
    "\n",
    "# concatenating the mel spectograms because vocoder is more efficient with longer ones\n",
    "mel = np.concatenate(mels, axis=1)\n",
    "\n",
    "# vocoder has callback func to display the generation. for now we will hide it\n",
    "no_action = lambda *args: None\n",
    "\n",
    "vocoder.infer_waveform(mel, target=200, overlap=50, progress_callback=no_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires a in_wav_fpath\n",
    "in_wav_fpath = os.path.join(project_root, 'example_wavs', 'originals', 'p240_00000.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the embedding, there are two methods to do this\n",
    "# directly loading from the file path\n",
    "preprocessed_wav = encoder.preprocess_wav(in_wav_fpath)\n",
    "\n",
    "# if the wav is already loaded\n",
    "original_wav, sampling_rate = librosa.load(str(in_wav_fpath))\n",
    "preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "# Now let's get the embedding\n",
    "embed = encoder.embed_utterance(preprocessed_wav)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs text (waiting on it)\n",
    "text = 'It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = 'Audio-books are a great alternative to reading books and online courses. MOOCs are great as they provide relevant information in a short period of time but they lack the in-depth coverage and comprehensiveness of the topic that books can provide. Books are the holy grail of information but in today’s fast-paced world they are cumbersome. So why not use your smartphone and make books accessible to you anytime you want? The objective of our project is to make books accessible on the fly and easy to understand. We use NLP techniques to convert a book to audio format, we also summarize the book if the user wants to try out a book before actually committing to it, or if they just want to get the gist of it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the synthesizer works in batch so the data must be in list or array\n",
    "# perfect because i have lots of text from summary\n",
    "texts = [text_2]\n",
    "embeds = [embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 30.870M\n",
      "+----------+---+\n",
      "| Tacotron | r |\n",
      "+----------+---+\n",
      "|   295k   | 2 |\n",
      "+----------+---+\n",
      " \n",
      "\n",
      "| Generating 1/1\n",
      "\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you know what attention layer alignments are, you can retrieve them from here\n",
    "specs = synthesizer.synthesize_spectrograms(texts, embeds)\n",
    "spec = specs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{| ████████████████ 437000/441600 | Batch Size: 46 | Gen Rate: 8.4kHz | }"
     ]
    }
   ],
   "source": [
    "generated_wav = vocoder.infer_waveform(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post generation\n",
    "generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_wav = encoder.preprocess_wav(generated_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, A = signal.butter(1, (0.1, 0.9), btype='bandpass', output='ba')\n",
    "smoothed_wav = signal.filtfilt(B, A, generated_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "out_filename = 'example_wavs/generated/training_voices_smoothed_01-09.wav'\n",
    "print(generated_wav.dtype)\n",
    "sf.write(out_filename, smoothed_wav.astype(np.float32), synthesizer.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0794e55eb06a81ddefb28a1c87f0f62207be250b0d705fe7dc45216952d1399"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
